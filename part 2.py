# import json
# import requests
# import re
# import urllib.parse
# import html
# import time
# from bs4 import BeautifulSoup
#
#
# class TelegramBot:
#     def __init__(self, token):
#         self.token = token
#         self.base_url = f"https://api.telegram.org/bot{token}/"
#
#     def get_updates(self, offset=None):
#         url = self.base_url + "getUpdates"
#         params = {
#             "timeout": 30,
#             "offset": offset
#         }
#         result = self._make_request(url, params, method="GET")
#         return json.loads(result) if result else None
#
#     def send_message(self, chat_id, text, reply_markup=None, parse_mode="HTML"):
#         url = self.base_url + "sendMessage"
#         data = {
#             "chat_id": chat_id,
#             "text": text,
#             "parse_mode": parse_mode
#         }
#         if reply_markup:
#             data["reply_markup"] = json.dumps(reply_markup)
#         return self._make_request(url, data)
#
#     def send_photo(self, chat_id, photo_url, caption="", reply_markup=None):
#         url = self.base_url + "sendPhoto"
#         data = {
#             "chat_id": chat_id,
#             "photo": photo_url,
#             "caption": caption,
#             "parse_mode": "HTML"
#         }
#         if reply_markup:
#             data["reply_markup"] = json.dumps(reply_markup)
#         return self._make_request(url, data)
#
#     def edit_message_reply_markup(self, chat_id, message_id, reply_markup=None):
#         url = self.base_url + "editMessageReplyMarkup"
#         data = {
#             "chat_id": chat_id,
#             "message_id": message_id
#         }
#         if reply_markup:
#             data["reply_markup"] = json.dumps(reply_markup)
#         return self._make_request(url, data)
#
#     def answer_callback_query(self, callback_query_id, text=None):
#         url = self.base_url + "answerCallbackQuery"
#         data = {
#             "callback_query_id": callback_query_id
#         }
#         if text:
#             data["text"] = text
#         return self._make_request(url, data)
#
#     @staticmethod
#     def _make_request(url, data=None, method="POST"):
#         try:
#             if method == "GET":
#                 response = requests.get(url, params=data, timeout=30)
#             else:
#                 response = requests.post(url, data=data, timeout=30)
#             return response.text
#         except Exception as e:
#             print(f"Request error: {e}")
#             return None
#
#
# class ItProgerParser:
#     def __init__(self):
#         self.base_url = "https://itproger.com/news"
#         self.session = requests.Session()
#         self.session.headers.update({
#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
#             'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
#             'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
#             'Referer': 'https://itproger.com/',
#         })
#
#     def get_news_page(self, page=1):
#         """Get news page from itproger.com"""
#         try:
#             if page > 1:
#                 url = f"{self.base_url}/{page}"
#             else:
#                 url = self.base_url
#
#             print(f"Loading: {url}")
#             response = self.session.get(url, timeout=15)
#             response.raise_for_status()
#
#             # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
#             with open(f"debug_page_{page}.html", "w", encoding="utf-8") as f:
#                 f.write(response.text)
#             print(f"‚úÖ Page saved to debug_page_{page}.html")
#
#             return response.text
#         except Exception as e:
#             print(f"‚ùå Page loading error: {e}")
#             return None
#
#     def parse_news_cards(self, html_content):
#         """Parse news cards from ItProger with multiple methods"""
#         if not html_content:
#             print("‚ùå No HTML content provided")
#             return []
#
#         soup = BeautifulSoup(html_content, 'html.parser')
#         articles = []
#
#         print("üîç Starting parsing with multiple methods...")
#
#         # –ú–µ—Ç–æ–¥ 1: –ò—â–µ–º —Å—Ç–∞—Ç—å–∏ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ ItProger
#         articles.extend(self._parse_by_articles(soup))
#
#         # –ú–µ—Ç–æ–¥ 2: –ò—â–µ–º –ø–æ –∫–∞—Ä—Ç–æ—á–∫–∞–º
#         articles.extend(self._parse_by_cards(soup))
#
#         # –ú–µ—Ç–æ–¥ 3: –ò—â–µ–º –ø–æ grid-—Å—Ç—Ä—É–∫—Ç—É—Ä–µ
#         articles.extend(self._parse_by_grid(soup))
#
#         # –ú–µ—Ç–æ–¥ 4: –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
#         articles.extend(self._parse_by_links(soup))
#
#         # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
#         unique_articles = []
#         seen_titles = set()
#         for article in articles:
#             if article['title'] and article['title'] not in seen_titles:
#                 unique_articles.append(article)
#                 seen_titles.add(article['title'])
#
#         print(f"‚úÖ Total unique articles found: {len(unique_articles)}")
#
#         # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
#         with open("debug_articles.json", "w", encoding="utf-8") as f:
#             json.dump(unique_articles, f, ensure_ascii=False, indent=2)
#
#         return unique_articles
#
#     def _parse_by_articles(self, soup):
#         """Parse using article tags"""
#         articles = []
#         news_articles = soup.find_all('article')
#         print(f"üîç Found {len(news_articles)} article tags")
#
#         for article in news_articles:
#             try:
#                 parsed = self._parse_article_element(article)
#                 if parsed and parsed['title']:
#                     articles.append(parsed)
#                     print(f"‚úÖ Article: {parsed['title'][:50]}...")
#             except Exception as e:
#                 continue
#         return articles
#
#     def _parse_by_cards(self, soup):
#         """Parse using card-like divs"""
#         articles = []
#
#         # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
#         card_selectors = [
#             'div[class*="news"]',
#             'div[class*="article"]',
#             'div[class*="post"]',
#             'div[class*="card"]',
#             'div[class*="item"]',
#             'div[class*="blog"]',
#             'div[class*="content"]'
#         ]
#
#         for selector in card_selectors:
#             cards = soup.select(selector)
#             print(f"üîç Selector '{selector}': found {len(cards)} elements")
#
#             for card in cards[:10]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 10
#                 try:
#                     parsed = self._parse_article_element(card)
#                     if parsed and parsed['title'] and len(parsed['title']) > 10:
#                         articles.append(parsed)
#                         print(f"‚úÖ Card: {parsed['title'][:50]}...")
#                 except Exception as e:
#                     continue
#
#         return articles
#
#     def _parse_by_grid(self, soup):
#         """Parse grid and flex containers"""
#         articles = []
#
#         # –ò—â–µ–º grid –∏ flex –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
#         grid_containers = soup.find_all(['div', 'section'], class_=lambda x: x and any(
#             word in str(x).lower() for word in ['grid', 'flex', 'row', 'container', 'wrapper']
#         ))
#
#         print(f"üîç Found {len(grid_containers)} grid/flex containers")
#
#         for container in grid_containers:
#             # –ò—â–µ–º –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ —ç–ª–µ–º–µ–Ω—Ç—ã —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
#             content_elems = container.find_all(['div', 'article', 'section'])
#             for elem in content_elems:
#                 try:
#                     parsed = self._parse_article_element(elem)
#                     if parsed and parsed['title'] and len(parsed['title']) > 10:
#                         articles.append(parsed)
#                         print(f"‚úÖ Grid item: {parsed['title'][:50]}...")
#                 except Exception as e:
#                     continue
#
#         return articles
#
#     def _parse_by_links(self, soup):
#         """Parse by finding news links"""
#         articles = []
# links
#         # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –≤–µ—Å—Ç–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏
#         news_links = soup.find_all('a', href=re.compile(r'/news/|/article/|/post/'))
#         print(f"üîç Found {len(news_links)} news-like links")
#
#         for link in news_links:
#             try:
#                 # –ü–æ–ª—É—á–∞–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π —ç–ª–µ–º–µ–Ω—Ç –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
#                 parent = link.find_parent(['div', 'article', 'li'])
#                 context_elem = parent if parent else link
#
#                 article = {
#                     'title': self._clean_text(link.get_text()),
#                     'url': self._make_absolute_url(link.get('href')),
#                     'image': '',
#                     'excerpt': '',
#                     'tags': []
#                 }
#
#                 # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä—è–¥–æ–º
#                 img = context_elem.find('img')
#                 if img:
#                     article['image'] = self._make_absolute_url(img.get('src') or img.get('data-src'))
#
#                 # –ò—â–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
#                 desc = context_elem.find('p') or context_elem.find('span')
#                 if desc:
#                     article['excerpt'] = self._clean_text(desc.get_text())[:150] + "..."
#
#                 if article['title'] and len(article['title']) > 10:
#                     articles.append(article)
#                     print(f"‚úÖ Link: {article['title'][:50]}...")
#
#             except Exception as e:
#                 continue
#
#         return articles
#
#     def _parse_article_element(self, elem):
#         """Parse individual article element"""
#         article = {
#             'title': '',
#             'image': '',
#             'excerpt': '',
#             'url': '',
#             'tags': []
#         }
#
#         # –ó–∞–≥–æ–ª–æ–≤–æ–∫ - –∏—â–µ–º –≤–æ –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ç–µ–≥–∞—Ö
#         title_elem = (
#                 elem.find('h1') or elem.find('h2') or elem.find('h3') or
#                 elem.find('h4') or elem.find(['a', 'span'], class_=lambda x: x and any(
#             word in str(x).lower() for word in ['title', 'head', 'name']
#         ))
#         )
#
#         if title_elem:
#             article['title'] = self._clean_text(title_elem.get_text())
#
#         # –°—Å—ã–ª–∫–∞
#         link_elem = elem.find('a', href=True)
#         if link_elem:
#             href = link_elem.get('href')
#             if href and ('/news/' in href or '/article/' in href):
#                 article['url'] = self._make_absolute_url(href)
#
#         # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
#         img_elem = elem.find('img')
#         if img_elem:
#             src = img_elem.get('src') or img_elem.get('data-src')
#             if src:
#                 article['image'] = self._make_absolute_url(src)
#
#         # –û–ø–∏—Å–∞–Ω–∏–µ
#         desc_elem = elem.find('p') or elem.find('div', class_=lambda x: x and any(
#             word in str(x).lower() for word in ['desc', 'text', 'content', 'excerpt', 'preview']
#         ))
#         if desc_elem:
#             text = self._clean_text(desc_elem.get_text())
#             if len(text) > 20:
#                 article['excerpt'] = text[:200] + "..." if len(text) > 200 else text
#
#         # –ï—Å–ª–∏ –Ω–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è, —Å–æ–∑–¥–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ
#         if not article['excerpt'] and article['title']:
#             article['excerpt'] = "IT-–Ω–æ–≤–æ—Å—Ç—å —Å ItProger. –ù–∞–∂–º–∏—Ç–µ –¥–ª—è —á—Ç–µ–Ω–∏—è –ø–æ–ª–Ω–æ–π –≤–µ—Ä—Å–∏–∏."
#
#         return article
#
#     def get_full_article(self, article_url):
#         """Get full article content"""
#         try:
#             print(f"üìñ Loading full article: {article_url}")
#             response = self.session.get(article_url, timeout=10)
#             response.raise_for_status()
#
#             soup = BeautifulSoup(response.text, 'html.parser')
#
#             # –ó–∞–≥–æ–ª–æ–≤–æ–∫
#             title = soup.find('h1')
#             title_text = self._clean_text(title.get_text()) if title else ""
#
#             # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
#             content_selectors = [
#                 'article',
#                 '.article-content',
#                 '.post-content',
#                 '.content',
#                 'main',
#                 '.news-content',
#                 '.entry-content'
#             ]
#
#             content_elem = None
#             for selector in content_selectors:
#                 content_elem = soup.select_one(selector)
#                 if content_elem:
#                     break
#
#             if not content_elem:
#                 content_elem = soup.find('body')
#
#             content_text = ""
#             if content_elem:
#                 # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
#                 for unwanted in content_elem.find_all(['script', 'style', 'nav', 'aside', 'header', 'footer']):
#                     unwanted.decompose()
#
#                 # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç
#                 paragraphs = content_elem.find_all(['p', 'h2', 'h3', 'li'])
#                 text_parts = []
#
#                 for p in paragraphs:
#                     text = self._clean_text(p.get_text())
#                     if len(text) > 20:
#                         if p.name in ['h2', 'h3']:
#                             text_parts.append(f"\nüîπ {text}\n")
#                         elif p.name == 'li':
#                             text_parts.append(f"‚Ä¢ {text}")
#                         else:
#                             text_parts.append(text)
#
#                 content_text = '\n'.join(text_parts)
#
#             result = f"<b>{title_text}</b>\n\n{content_text}" if title_text else content_text
#             return result[:3800] + "..." if len(result) > 3800 else result
#
#         except Exception as e:
#             print(f"‚ùå Error loading full article: {e}")
#             return "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏. –ü–æ—Å–µ—Ç–∏—Ç–µ —Å–∞–π—Ç –¥–ª—è —á—Ç–µ–Ω–∏—è."
#
#     @staticmethod
#     def _make_absolute_url(url):
#         """Convert relative URL to absolute"""
#         if not url:
#             return ""
#         if url.startswith('//'):
#             return 'https:' + url
#         elif url.startswith('/'):
#             return 'https://itproger.com' + url
#         elif not url.startswith('http'):
#             return 'https://itproger.com/' + url
#         return url
#
#     @staticmethod
#     def _clean_text(text):
#         """Clean text from extra spaces"""
#         if not text:
#             return ""
#         text = html.unescape(text)
#         text = re.sub(r'\s+', ' ', text)
#         text = re.sub(r'\n+', '\n', text)
#         text = text.strip()
#         return text
#
#
# def get_main_menu_keyboard():
#     """Main menu keyboard"""
#     return {
#         "keyboard": [
#             [{"text": "üì∞ –í—Å–µ –Ω–æ–≤–æ—Å—Ç–∏"}],
#             [{"text": "üîÑ –û–±–Ω–æ–≤–∏—Ç—å"}, {"text": "üÜò –ü–æ–º–æ—â—å"}]
#         ],
#         "resize_keyboard": True,
#         "one_time_keyboard": False
#     }
#
#
# def get_news_navigation_keyboard(page, has_next=True):
#     """News navigation inline keyboard"""
#     buttons = []
#
#     if page > 1:
#         buttons.append({"text": "‚óÄÔ∏è –ù–∞–∑–∞–¥", "callback_data": f"news_page_{page - 1}"})
#
#     buttons.append({"text": f"üìÑ {page}", "callback_data": "current_page"})
#
#     if has_next:
#         buttons.append({"text": "–í–ø–µ—Ä–µ–¥ ‚ñ∂Ô∏è", "callback_data": f"news_page_{page + 1}"})
#
#     return {"inline_keyboard": [buttons]}
#
#
# def get_article_keyboard(article_url, page, article_index, total_articles):
#     """Article inline keyboard"""
#     return {
#         "inline_keyboard": [
#             [
#                 {"text": "üìñ –ß–∏—Ç–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é", "callback_data": f"full_article_{article_url}"},
#                 {"text": "üåê –ù–∞ —Å–∞–π—Ç", "url": article_url}
#             ],
#             [
#                 {"text": "‚¨ÖÔ∏è –ü—Ä–µ–¥—ã–¥—É—â–∞—è",
#                  "callback_data": f"article_{page}_{article_index - 1}"} if article_index > 0 else {"text": "‚èπÔ∏è",
#                                                                                                     "callback_data": "none"},
#                 {"text": f"{article_index + 1}/{total_articles}", "callback_data": "current_article"},
#                 {"text": "–°–ª–µ–¥—É—é—â–∞—è ‚û°Ô∏è",
#                  "callback_data": f"article_{page}_{article_index + 1}"} if article_index < total_articles - 1 else {
#                     "text": "‚èπÔ∏è", "callback_data": "none"}
#             ],
#             [
#                 {"text": "üîô –ö —Å–ø–∏—Å–∫—É —Å—Ç—Ä–∞–Ω–∏—Ü", "callback_data": f"news_page_{page}"}
#             ]
#         ]
#     }
#
#
# def format_article_card(article, index, total):
#     """Format article card for sending"""
#     caption = f"<b>üì∞ {article['title']}</b>\n\n"
#
#     if article['excerpt']:
#         caption += f"{article['excerpt']}\n\n"
#
#     caption += f"üìä –ù–æ–≤–æ—Å—Ç—å {index + 1} –∏–∑ {total}"
#     return caption
#
#
# def handle_message(bot, parser, message):
#     chat_id = message['chat']['id']
#     text = message.get('text', '')
#
#     if text in ['/start', 'üöÄ –°—Ç–∞—Ä—Ç']:
#         welcome_text = (
#             "üëã <b>–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ IT News Parser Bot!</b>\n\n"
#             "üì∞ <b>–ü–æ–ª—É—á–∞–π—Ç–µ –≤—Å–µ IT-–Ω–æ–≤–æ—Å—Ç–∏ —Å ItProger</b>\n\n"
#             "üîπ <b>–§—É–Ω–∫—Ü–∏–∏:</b>\n"
#             "‚Ä¢ –í—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å itproger.com/news\n"
#             "‚Ä¢ –ü—Ä–æ—Å–º–æ—Ç—Ä –≤—Å–µ—Ö —Ç–µ–≥–æ–≤ –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ\n"
#             "‚Ä¢ –ù–∞–≤–∏–≥–∞—Ü–∏—è –º–µ–∂–¥—É –Ω–æ–≤–æ—Å—Ç—è–º–∏\n"
#             "‚Ä¢ –ü—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–∞–π—Ç\n"
#             "‚Ä¢ –ü–æ–ª–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã —Å—Ç–∞—Ç–µ–π\n\n"
#             "üöÄ <b>–ù–∞–∂–º–∏—Ç–µ '–í—Å–µ –Ω–æ–≤–æ—Å—Ç–∏' —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏!</b>"
#         )
#         bot.send_message(chat_id, welcome_text, get_main_menu_keyboard())
#
#     elif text in ['üì∞ –í—Å–µ –Ω–æ–≤–æ—Å—Ç–∏', 'üîÑ –û–±–Ω–æ–≤–∏—Ç—å']:
#         bot.send_message(chat_id, "‚è≥ –ó–∞–≥—Ä—É–∂–∞—é –≤—Å–µ IT-–Ω–æ–≤–æ—Å—Ç–∏ —Å ItProger...")
#         show_news_page(bot, parser, chat_id, 1)
#
#     elif text in ['üÜò –ü–æ–º–æ—â—å', '/help']:
#         help_text = (
#             "üìñ <b>–ü–æ–º–æ—â—å –ø–æ IT News Parser Bot</b>\n\n"
#             "ü§ñ <b>–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:</b>\n"
#             "1. –ù–∞–∂–º–∏—Ç–µ <b>–í—Å–µ –Ω–æ–≤–æ—Å—Ç–∏</b>\n"
#             "2. –ü—Ä–æ—Å–º–∞—Ç—Ä–∏–≤–∞–π—Ç–µ –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏\n"
#             "3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–Ω–æ–ø–∫–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –º–µ–∂–¥—É –Ω–æ–≤–æ—Å—Ç—è–º–∏\n"
#             "4. –ù–∞–∂–º–∏—Ç–µ <b>–ß–∏—Ç–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é</b> –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n"
#             "5. –ù–∞–∂–º–∏—Ç–µ <b>–ù–∞ —Å–∞–π—Ç</b> –¥–ª—è –æ—Ç–∫—Ä—ã—Ç–∏—è –Ω–∞ ItProger\n\n"
#             "üîÑ <b>–û–±–Ω–æ–≤–∏—Ç—å</b> - –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏\n\n"
#             "üí° <b>–ò—Å—Ç–æ—á–Ω–∏–∫:</b> itproger.com/news"
#         )
#         bot.send_message(chat_id, help_text, get_main_menu_keyboard())
#
#     else:
#         unknown_text = (
#             "‚ùå <b>–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞</b>\n\n"
#             "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–Ω–æ–ø–∫–∏ –º–µ–Ω—é –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏.\n\n"
#             "–î–ª—è —Å–ø—Ä–∞–≤–∫–∏ –Ω–∞–∂–º–∏—Ç–µ <b>üÜò –ü–æ–º–æ—â—å</b>"
#         )
#         bot.send_message(chat_id, unknown_text, get_main_menu_keyboard())
#
#
# def handle_callback(bot, parser, callback_query):
#     chat_id = callback_query['message']['chat']['id']
#     message_id = callback_query['message']['message_id']
#     callback_data = callback_query['data']
#     callback_id = callback_query['id']
#
#     if callback_data.startswith('news_page_'):
#         page = int(callback_data.split('_')[-1])
#         bot.answer_callback_query(callback_id, f"–ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}...")
#         show_news_page(bot, parser, chat_id, page, message_id)
#
#     elif callback_data.startswith('article_'):
#         parts = callback_data.split('_')
#         page = int(parts[1])
#         article_index = int(parts[2])
#         bot.answer_callback_query(callback_id, f"–ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –Ω–æ–≤–æ—Å—Ç–∏ {article_index + 1}...")
#         show_specific_article(bot, parser, chat_id, page, article_index, message_id)
#
#     elif callback_data.startswith('full_article_'):
#         article_url = callback_data.replace('full_article_', '')
#         bot.answer_callback_query(callback_id, "–ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç...")
#         show_full_article(bot, parser, chat_id, article_url)
#
#     elif callback_data in ['current_page', 'current_article', 'none']:
#         bot.answer_callback_query(callback_id, "")
#
#
# def show_news_page(bot, parser, chat_id, page, edit_message_id=None):
#     """Show news page with all articles"""
#     if edit_message_id:
#         bot.edit_message_reply_markup(chat_id, edit_message_id)
#
#     html_content = parser.get_news_page(page)
#
#     if not html_content:
#         bot.send_message(chat_id,
#                          "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏.\n"
#                          "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.",
#                          get_main_menu_keyboard())
#         return
#
#     articles = parser.parse_news_cards(html_content)
#
#     if not articles:
#         bot.send_message(chat_id,
#                          "üì≠ –ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.\n\n"
#                          "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n"
#                          "‚Ä¢ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É\n"
#                          "‚Ä¢ –ù–∞–∂–∞—Ç—å <b>üîÑ –û–±–Ω–æ–≤–∏—Ç—å</b>\n"
#                          "‚Ä¢ –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ø–æ–∑–∂–µ\n\n"
#                          "–ë–æ—Ç —Å–æ—Ö—Ä–∞–Ω–∏–ª HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏.",
#                          get_main_menu_keyboard())
#         return
#
#     # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—É—é –Ω–æ–≤–æ—Å—Ç—å
#     show_specific_article(bot, parser, chat_id, page, 0)
#
#     # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
#     nav_text = f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page} - –ù–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π"
#     nav_keyboard = get_news_navigation_keyboard(page, has_next=True)
#     bot.send_message(chat_id, nav_text, nav_keyboard)
#
#
# def show_specific_article(bot, parser, chat_id, page, article_index, edit_message_id=None):
#     """Show specific article by index"""
#     if edit_message_id:
#         bot.edit_message_reply_markup(chat_id, edit_message_id)
#
#     # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞–Ω–æ–≤–æ –¥–ª—è —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
#     html_content = parser.get_news_page(page)
#     if not html_content:
#         bot.send_message(chat_id, "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É")
#         return
#
#     articles = parser.parse_news_cards(html_content)
#     if not articles or article_index >= len(articles):
#         bot.send_message(chat_id, "‚ùå –ù–æ–≤–æ—Å—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
#         return
#
#     article = articles[article_index]
#     caption = format_article_card(article, article_index, len(articles))
#     keyboard = get_article_keyboard(article['url'], page, article_index, len(articles))
#
#     if article.get('image') and article['image'].startswith('http'):
#         bot.send_photo(chat_id, article['image'], caption, keyboard)
#     else:
#         bot.send_message(chat_id, caption, keyboard)
#
#
# def show_full_article(bot, parser, chat_id, article_url):
#     """Show full article content"""
#     content = parser.get_full_article(article_url)
#
#     message = f"<b>üìñ –ü–æ–ª–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å</b>\n\n{content}"
#
#     back_button = {
#         "inline_keyboard": [
#             [
#                 {"text": "üåê –û—Ç–∫—Ä—ã—Ç—å –Ω–∞ —Å–∞–π—Ç–µ", "url": article_url},
#                 {"text": "üîô –ù–∞–∑–∞–¥ –∫ –Ω–æ–≤–æ—Å—Ç—è–º", "callback_data": "news_page_1"}
#             ]
#         ]
#     }
#
#     if len(message) > 4096:
#         parts = [message[i:i + 4000] for i in range(0, len(message), 4000)]
#         for part in parts[:-1]:
#             bot.send_message(chat_id, part)
#         bot.send_message(chat_id, parts[-1], back_button)
#     else:
#         bot.send_message(chat_id, message, back_button)
#
#
# def main():
#     token = "8453465834:AAEdzYPmtmjayAtTfhitVaiiA3aBtELRDQY"
#     bot = TelegramBot(token)
#     parser = ItProgerParser()
#
#     print("üöÄ IT News Parser Bot Started")
#     print("üì∞ Source: itproger.com/news")
#     print("ü§ñ Bot: @parsing07_bot")
#     print("üì° Monitoring for messages...")
#
#     last_update_id = None
#
#     while True:
#         try:
#             updates = bot.get_updates(offset=last_update_id)
#
#             if updates and 'result' in updates:
#                 for update in updates['result']:
#                     last_update_id = update['update_id'] + 1
#
#                     if 'message' in update:
#                         user_text = update['message'].get('text', '')
#                         print(f"üì® Message from {update['message']['chat']['id']}: {user_text}")
#                         handle_message(bot, parser, update['message'])
#
#                     elif 'callback_query' in update:
#                         print(f"üîÑ Callback: {update['callback_query']['data']}")
#                         handle_callback(bot, parser, update['callback_query'])
#
#             time.sleep(1)
#
#         except KeyboardInterrupt:
#             print("\nüõë Bot stopped")
#             break
#         except Exception as e:
#             print(f"Error: {e}")
#             time.sleep(5)
#
#
# if __name__ == "__main__":
#     main()